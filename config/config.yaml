# ===========================================
# Agentic RAG System - Production Configuration
# ===========================================
# This configuration uses Milvus Cloud with HNSW indexing

# ===========================================
# LLM Configuration (OpenAI Only)
# ===========================================
llm:
  provider: "openai"
  model: "gpt-4o-mini" # Options: gpt-4o, gpt-4o-mini, gpt-4-turbo
  temperature: 0.2
  max_tokens: 4096
  top_p: 1.0
  timeout: 60
  retry_attempts: 3
  retry_delay: 2.0
  # Function calling
  enable_function_calling: true
  # Streaming
  enable_streaming: true

# ===========================================
# Embedding Configuration (OpenAI Only)
# ===========================================
embedding:
  provider: "openai"
  model: "text-embedding-3-small" # 1536 dimensions
  # Alternative: text-embedding-3-large (3072 dimensions)
  dimension: 1536
  batch_size: 100
  normalize: true
  # Caching
  cache_enabled: true
  cache_size: 10000

# ===========================================
# Vector Database (Milvus Cloud - Production)
# ===========================================
vector_db:
  provider: "milvus_cloud"
  collection_name: "documents"

  # Milvus Cloud (Zilliz) Connection
  # Set these in environment variables:
  # MILVUS_URI or ZILLIZ_URI
  # MILVUS_TOKEN or ZILLIZ_TOKEN

  # Schema
  embedding_dimension: 1536
  id_max_length: 256
  content_max_length: 65535

  # HNSW Index Configuration (Optimal for production)
  index_type: "HNSW"
  metric_type: "COSINE" # Best for normalized embeddings

  # HNSW Parameters
  hnsw:
    m: 32 # Higher M = better recall, more memory (16-64)
    ef_construction: 360 # Higher = better index quality (100-500)
    ef_search: 128 # Higher = better search quality (64-256)

  # Consistency
  consistency_level: "Strong" # Strong, Bounded, Eventually

  # Retry
  max_retries: 3
  retry_delay: 1.0

# ===========================================
# OCR Configuration (EasyOCR + spaCy)
# ===========================================
ocr:
  enabled: true
  engine: "easyocr"

  # Language support (multilingual)
  languages:
    - "en"
    - "hi" # Hindi
    - "de" # German
    - "fr" # French
    - "es" # Spanish

  # Processing
  dpi: 300
  enhance_contrast: true
  detect_rotation: true
  paragraph_mode: true
  detail: 1 # 0=fast, 1=accurate

  # GPU acceleration (set based on hardware)
  gpu: false

  # Confidence thresholds
  min_confidence: 0.3
  low_confidence_threshold: 0.5

  # NLP Post-processing (spaCy)
  nlp:
    enabled: true
    model: "en_core_web_sm"
    extract_entities: true
    extract_keywords: true
    max_keywords: 20

# ===========================================
# Advanced Chunking Configuration
# ===========================================
chunking:
  strategy: "semantic" # recursive, semantic, agentic
  chunk_size: 512
  chunk_overlap: 100
  min_chunk_size: 100
  max_chunk_size: 1500

  # Semantic chunking
  semantic_threshold: 0.5

  # Cross-reference linkage (IMPORTANT)
  enable_cross_reference: true
  link_tables_to_text: true
  link_images_to_text: true
  context_window: 2 # paragraphs before/after

  # Hierarchical chunking (parent-child)
  enable_hierarchy: true
  parent_chunk_size: 2000
  parent_overlap: 200

  # Separators (in order of priority)
  separators:
    - "\n\n" # Paragraphs
    - "\n" # Lines
    - ". " # Sentences
    - "! "
    - "? "
    - "; "
    - ", "
    - " "

# ===========================================
# Advanced Retrieval Configuration
# ===========================================
retrieval:
  top_k: 10
  rerank_top_k: 5
  similarity_threshold: 0.3

  # Fusion Method
  fusion_method: "rrf" # rrf, weighted
  rrf_k: 60

  # Re-ranking (Cross-encoder)
  enable_rerank: true
  rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

  # Diversity (MMR)
  enable_diversity: true
  diversity_lambda: 0.5 # Balance relevance/diversity

  # BM25 Keyword Search
  enable_bm25: true
  bm25_k1: 1.5
  bm25_b: 0.75
  bm25_weight: 0.3
  dense_weight: 0.7

  # Fuzzy Matching
  enable_fuzzy: true
  fuzzy_threshold: 0.7

  # Multi-Query Retrieval
  enable_multi_query: true
  num_query_variations: 3

  # Query Expansion
  enable_query_expansion: false

  # Context Compression
  enable_compression: true
  max_context_tokens: 4000

# ===========================================
# Web Search Configuration
# ===========================================
web_search:
  enabled: true
  provider: "duckduckgo" # duckduckgo, serper, tavily
  max_results: 5
  timeout: 10

  # Result processing
  include_snippets: true
  filter_adult: true

  # Provider-specific (if using Serper/Tavily)
  # api_key set via SERPER_API_KEY or TAVILY_API_KEY

# ===========================================
# CrewAI Agent Configuration
# ===========================================
agents:
  process: "hierarchical" # sequential, hierarchical
  verbose: true
  max_iterations: 5
  max_rpm: 10

  # Memory Configuration
  memory:
    enabled: true
    type: "hybrid" # short_term, long_term, entity, hybrid
    short_term_max_items: 100
    short_term_ttl: 3600 # 1 hour
    long_term_enabled: true
    long_term_collection: "memory"
    entity_enabled: true
    max_history_turns: 10

  # Caching
  cache_responses: true

  # Retry
  fallback_enabled: true
  max_retries: 2

  # Callbacks
  enable_callbacks: true
  log_agent_steps: true

  # Individual agent toggles
  enable_supervisor: true
  enable_retriever: true
  enable_generator: true
  enable_feedback: true

# ===========================================
# Data Pipeline Configuration
# ===========================================
data_pipeline:
  input_directory: "./data/raw"
  processed_directory: "./data/processed"

  batch_size: 10
  max_file_size_mb: 50

  # Supported file types
  supported_extensions:
    - ".pdf"
    - ".docx"
    - ".doc"
    - ".txt"
    - ".md"
    - ".html"
    - ".htm"
    - ".xlsx"
    - ".xls"
    - ".csv"
    - ".pptx"
    - ".ppt"
    - ".json"
    - ".xml"
    - ".png"
    - ".jpg"
    - ".jpeg"
    - ".gif"
    - ".bmp"
    - ".tiff"
    - ".webp"

  # Deduplication
  enable_dedup: true

  # Table extraction
  extract_tables: true
  table_extraction_method: "camelot" # camelot, tabula

  # Image extraction
  extract_images: true
  image_description: true

# ===========================================
# API Configuration
# ===========================================
api:
  host: "0.0.0.0"
  port: 8000
  debug: false
  workers: 1

  # Rate limiting
  rate_limit:
    enabled: true
    requests_per_minute: 60
    requests_per_hour: 1000

  # Timeouts
  request_timeout: 120

  # CORS
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_credentials: true
    allow_methods: ["*"]
    allow_headers: ["*"]

  # API versioning
  version: "v1"
  prefix: "/api/v1"

# ===========================================
# Logging Configuration
# ===========================================
logging:
  level: "INFO" # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # File logging
  file:
    enabled: true
    path: "./logs/app.log"
    max_size_mb: 10
    backup_count: 5

  # Specific loggers
  loggers:
    httpx: "WARNING"
    httpcore: "WARNING"
    urllib3: "WARNING"
    crewai: "INFO"
    openai: "WARNING"

# ===========================================
# Monitoring Configuration
# ===========================================
monitoring:
  prometheus:
    enabled: false
    port: 9090

  tracing:
    enabled: true
    sample_rate: 1.0

  # Health checks
  health_check:
    enabled: true
    interval: 30

# ===========================================
# Security Configuration
# ===========================================
security:
  # API authentication (optional)
  api_auth:
    enabled: false
    type: "bearer" # bearer, api_key
    header: "Authorization"

  # Rate limiting by IP
  ip_rate_limit:
    enabled: true
    max_requests: 100
    window_seconds: 60
